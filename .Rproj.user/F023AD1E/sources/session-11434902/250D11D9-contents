# Trim the influential data points (those with CD > .2)


remove_outliers = function(word_type)
{
p_data = read.csv(here("data", "tidy", "pilot_data.csv")) %>% 
  filter(type == word_type)
model_check = lmer(log_rt ~ 1 + (1 | source) + (1 | word), data = p_data)
pf = cooks.distance(model_check) %>% 
  as.data.frame() %>% 
  rownames_to_column("index") 

p_data$index = c(1:nrow(p_data))
p_data$index = as.character(p_data$index)

pf %>% 
  left_join(p_data, by = "index") %>% 
  filter(. < .2) %>% 
  ggplot(aes(y = ., x = as.numeric(index), label = word)) + 
  geom_text(size = 3.5) +
  theme_minimal() + 
  xlab("Index") + 
  ylab("Leverage") + 
  ggtitle("Each data point's on the intercept only model")

trim = pf %>% 
  left_join(p_data, by = "index") %>% 
  filter(. < .2) 


mean(p_data$key_resp_lextale_trial.rt)
mean(trim$key_resp_lextale_trial.rt)

return(trim)
}

unique(p_data$type)

three = remove_outliers("three_way_cognate")
two = remove_outliers("two_way_cognate")
nc = remove_outliers("non_cognate")

trimmed_df = rbind(three,two,nc)

# re run model 

model_t = lmerTest::lmer(log_rt ~ type + (1 | source) + (1 | word), data = trimmed_df)

summary(model_t) # There is an effect for two-way cognates, but not 3 




