## Tidy data 


#other_info = read.csv(here("data", "stimuli", "stim.csv")) %>% 
#  select(word, length, Subtlex_ES_log10.abs.1.)

tidy_data <- fs::dir_ls(here("data", "pilot_data"),
                     regexp = "\\.csv$") %>%
  map_dfr(read_csv, .id = "source", 
          col_types = cols(.default = "c")) %>% 
   mutate(source = str_remove(source, "/Users/kyleparrish/Documents/GitHub/l3_cognates copy/data/pilot_data/")) %>% 
  mutate(source = substr(source, 1, 10)) %>% 
  select(source, word, type, correct_reponse, key_resp_lextale_trial.keys, key_resp_lextale_trial.corr, key_resp_lextale_trial.rt) %>% 
  filter(!is.na(key_resp_lextale_trial.keys)) %>% 
  mutate(is_correct =
           ifelse(
             correct_reponse == 1 & key_resp_lextale_trial.keys == 1 | 
               correct_reponse == 0 & key_resp_lextale_trial.keys == 0,1,0)) %>% 
  filter(key_resp_lextale_trial.rt < 2 & key_resp_lextale_trial.rt > .5)  %>% 
  filter(is_correct == 1)

 # left_join(other_info, by = "word")


tidy_data$key_resp_lextale_trial.rt = as.numeric(tidy_data$key_resp_lextale_trial.rt)
tidy_data$log_rt = log(tidy_data$key_resp_lextale_trial.rt)

  
## Correctness as a function of word type 
cdf = tidy_data %>% 
  group_by(source) %>% 
  summarize(no_correct = sum(is_correct)) %>% 
  filter(no_correct > 70) 

unique(cdf$source)


cdf %>% 
  ggplot(aes(x = source, y = no_correct, fill = source)) + geom_col()

#cdf$totals = c(24,48,12,12)
#cdf$pct = cdf$no_correct/cdf$totals


#outlier = tidy_data %>% 
#  filter(word != "fauna")

cdf_m = tidy_data %>% 
  filter(source %in% unique(cdf$source)) %>% 
  group_by(type,source) %>% 
  summarize(mean_rt = mean(key_resp_lextale_trial.rt))

cdf_m %>% 
  filter(type == "non_cognate" | type == "two_way_cognate" | type == "three_way_cognate") %>% 
  ggplot(aes(x = type, y = mean_rt, fill = type)) + geom_col(position = "dodge") +
  facet_wrap(~source)

cdf_m %>% 
  filter(type == "non_cognate" | type == "two_way_cognate" | type == "three_way_cognate") %>% 
  ggplot(aes(x = type, y = mean_rt, fill = type)) + geom_col(position = "dodge") 

model_data = tidy_data %>% 
  filter(source %in% unique(cdf$source))

library(lmerTest)

model = lme4::lmer(key_resp_lextale_trial.rt ~ type + (1 | source) + (1 | word), data = model_data)

summary(model)

plot(cooks.distance(model))


ppt_list = unique(tidy_data$source)
nested_list = list()
for (i in 1:length(ppt_list)) {
thisdata = tidy_data %>% filter(source == ppt_list[i])
mod = glm(log_rt ~ type, data = thisdata)
coefs = summary(mod)

cognate_eff = ifelse(coefs[["coefficients"]][3,1] & coefs[["coefficients"]][4,1] < 0, 1, 0)
both_sig = ifelse(coefs[["coefficients"]][3,4] & coefs[["coefficients"]][4,4] < 0.05, 1, 0)

thisrun_data = data.frame(ppt = ppt_list[i], cognate_eff, both_sig)
nested_list[[i]] = thisrun_data
}

list_together = do.call(rbind, nested_list)



# Function to check for influential data points
find_outliers = function(df) {  
  model_check = lm(key_resp_lextale_trial.rt ~ 1, data = df)
  pf = cooks.distance(model_check) %>% 
    as.data.frame() %>% 
    rownames_to_column("index") 
  
  df$index = c(1:nrow(df))
  df$index = as.character(df$index)
  
  p = pf %>% 
    left_join(df, by = "index") %>% 
    ggplot(aes(y = ., x = as.numeric(index), label = word)) + 
    geom_text(size = 3.5) +
    theme_minimal() + 
    xlab("Index") + 
    ylab("Leverage") + 
    ggtitle("Each data point's on the intercept only model")
  return(p)
}

find_outliers(tidy_data)

ppt = "pilot_1.cs"
thisdata = model_data %>% filter(source == 
                                   ppt) %>% 
  filter(type == "two_way_cognate") 
find_outliers(thisdata)
ppt_list[i]


